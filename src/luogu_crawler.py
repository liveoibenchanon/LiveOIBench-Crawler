import time
import math
import json
import re
from collections import defaultdict
from selenium import webdriver
from selenium.webdriver.edge.options import Options
from selenium.webdriver.edge.service import Service
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
from concurrent.futures import ProcessPoolExecutor

def extract_problem_sort_key(title):
    match = re.search(r"#(\d+)", title)
    if match:
        return (0, int(match.group(1)))  # ä¼˜å…ˆæ’åºé¡¹1ï¼šé¢˜å·æ•°å­—
    return (1, title)  # ä¼˜å…ˆæ’åºé¡¹2ï¼šæ ‡é¢˜å­—ç¬¦ä¸²

def setup_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    service = Service(executable_path="C:\\edgedriver_win32\\msedgedriver.exe")
    return webdriver.Edge(service=service, options=options)

def get_max_pages(keyword):
    driver = setup_driver()
    url = f"https://www.luogu.com.cn/problem/list?keyword={keyword}"
    driver.get(url)
    time.sleep(2)

    try:
        soup = BeautifulSoup(driver.page_source, "html.parser")
        count_tag = soup.select_one(".result-count .number")
        total = int(count_tag.text.strip())
        max_page = math.ceil(total / 50)
        print(f"ğŸ”¢ å…³é”®è¯ {keyword} å…± {total} é¢˜ï¼Œçº¦ä¸º {max_page} é¡µï¼ˆæ¯é¡µ50é¢˜ï¼‰")
    except Exception as e:
        print("âš ï¸ è·å–é¢˜ç›®æ€»æ•°å¤±è´¥ï¼Œé»˜è®¤è®¾ä¸º1é¡µ:", e)
        max_page = 1

    driver.quit()
    return max_page


def scrape_luogu_problems(keyword, pages=1):
    driver = setup_driver()
    structured_data = defaultdict(lambda: defaultdict(list))
    for page in range(1, pages + 1):
        url = f"https://www.luogu.com.cn/problem/list?keyword={keyword}&page={page}"
        print(f"\n--- è®¿é—®ç¬¬ {page} é¡µ: {url}")
        driver.get(url)
        time.sleep(2)

        # Step 1: è·å–æ¥æºæ ‡ç­¾ + éš¾åº¦
        source_info = {}
        rows = driver.find_elements(By.CSS_SELECTOR, ".row-wrap .row")
        print(f"æ‰¾åˆ° {len(rows)} ä¸ªé¢˜ç›®è¡Œ")
        for row in rows:
            try:
                title_el = row.find_element(By.CSS_SELECTOR, ".title a")
                href = title_el.get_attribute("href")
                pid = href.split("/")[-1]

                tags = [t.text.strip() for t in row.find_elements(By.CSS_SELECTOR, ".tags .tag span")]

                # è·å–éš¾åº¦
                try:
                    difficulty = row.find_element(By.CSS_SELECTOR, ".difficulty span").text.strip()
                except:
                    difficulty = ""

                source_info[pid] = {
                    "æ ‡é¢˜": title_el.text.strip(),
                    "é“¾æ¥": href,
                    "æ¥æºæ ‡ç­¾": tags,
                    "éš¾åº¦": difficulty
                }

                print(f"[æ¥æº] {pid}: æ ‡ç­¾={tags} éš¾åº¦={difficulty}")
            except Exception as e:
                print("è·³è¿‡æŸè¡Œï¼ˆå¯èƒ½ç»“æ„å¼‚å¸¸ï¼‰:", e)

        # Step 2: ç‚¹å‡»å…¨å±€â€œæ˜¾ç¤ºç®—æ³•â€æŒ‰é’®
        try:
            show_btn = driver.find_element(By.LINK_TEXT, "æ˜¾ç¤ºç®—æ³•")
            driver.execute_script("arguments[0].click();", show_btn)
            print("å·²ç‚¹å‡»â€˜æ˜¾ç¤ºç®—æ³•â€™ï¼Œç­‰å¾…æ ‡ç­¾åŠ è½½...")
            time.sleep(1)
        except:
            print("æ²¡æœ‰æ‰¾åˆ°â€˜æ˜¾ç¤ºç®—æ³•â€™æŒ‰é’®ï¼Œè·³è¿‡ç®—æ³•æ ‡ç­¾")

        # Step 3: è·å–ç®—æ³•æ ‡ç­¾
        algo_info = {}
        rows = driver.find_elements(By.CSS_SELECTOR, ".row-wrap .row")
        for row in rows:
            try:
                title_el = row.find_element(By.CSS_SELECTOR, ".title a")
                href = title_el.get_attribute("href")
                pid = href.split("/")[-1]
                tags = [t.text.strip() for t in row.find_elements(By.CSS_SELECTOR, ".tags .tag span")]
                algo_info[pid] = tags
                print(f"[ç®—æ³•] {pid}: {tags}")
            except:
                continue

        # Step 4: åˆå¹¶ç»“æœå¹¶å­˜å…¥ structured_dataï¼ˆæ–°å¢ï¼‰
        for pid, info in source_info.items():
            algo_tags = algo_info.get(pid, [])

            # æå–å¹´ä»½
            year = None
            for tag in info["æ¥æºæ ‡ç­¾"]:
                match = re.match(r"\d{4}", tag)
                if match:
                    year = match.group()
                    break

            # æå–æ¯”èµ›åï¼ˆå¦‚ COCIï¼‰
            comp = None
            for tag in info["æ¥æºæ ‡ç­¾"]:
                if not any(c.isdigit() for c in tag):
                    comp = tag.split("ï¼ˆ")[0].strip()
                    break

            if not comp or not year:
                print(f"âš ï¸ æ¥æºæ ‡ç­¾ä¸­æœªè¯†åˆ«åˆ°æ¯”èµ›/å¹´ä»½ï¼Œå°è¯•ä»æ ‡é¢˜ä¸­æå–: {info['æ ‡é¢˜']}")
                # æ¯”èµ›å = [] ä¸­çš„å­—æ¯éƒ¨åˆ†
                match_comp = re.search(r"\[([A-Za-z]+)", info["æ ‡é¢˜"])
                if match_comp:
                    comp = match_comp.group(1).upper()

                # å¹´ä»½ = [] ä¸­æœ€å¤§çš„4ä½æ•°å­—
                match_years = re.findall(r"\d{4}", info["æ ‡é¢˜"])
                if match_years:
                    year = max(match_years)

                if not comp or not year:
                    print(f"âŒ æ— æ³•ä»æ ‡é¢˜ä¸­æå–æ¯”èµ›æˆ–å¹´ä»½ï¼Œè·³è¿‡: {pid}")
                    continue

            structured_data[comp][year].append({
                "æ ‡é¢˜": info["æ ‡é¢˜"],
                "éš¾åº¦": info["éš¾åº¦"],
                "ç®—æ³•æ ‡ç­¾": algo_tags
            })

            print(f"\nğŸ“Œ ç»¼åˆä¿¡æ¯ - {pid}")
            print(f"æ ‡é¢˜: {info['æ ‡é¢˜']}")
            print(f"é“¾æ¥: {info['é“¾æ¥']}")
            print(f"æ¥æºæ ‡ç­¾: {', '.join(info['æ¥æºæ ‡ç­¾'])}")
            print(f"éš¾åº¦: {info['éš¾åº¦']}")
            print(f"ç®—æ³•æ ‡ç­¾: {', '.join(algo_tags)}")

    driver.quit()
    print("\nâœ… æ‰€æœ‰é¡µå¤„ç†å®Œæˆã€‚")


    sorted_output = {
        comp: {
            year: sorted(problem_list, key=lambda x: extract_problem_sort_key(x["æ ‡é¢˜"]))
            for year, problem_list in sorted(years.items(), key=lambda item: int(item[0]))
        }
        for comp, years in structured_data.items()
    }

    print("\nğŸ“¦ æ•´ç†å®Œæˆçš„ç»“æ„åŒ–æ•°æ®ï¼š")
    print(json.dumps(sorted_output, indent=2, ensure_ascii=False))

    with open(f"./jsons/{keyword}_problems.json", "w", encoding="utf-8") as f:
        json.dump(sorted_output, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ å·²ä¿å­˜ä¸ºæ–‡ä»¶ï¼š{keyword}_problems.json")

def crawl_keyword(keyword):
    max_page = get_max_pages(keyword)
    scrape_luogu_problems(keyword=keyword, pages=max_page)

# ç¤ºä¾‹è°ƒç”¨
if __name__ == "__main__":
    #keyword = "CCO"
    keywords = ["APIO", "BalticOI", "CCO", "CCC", "CEOI", "EGOI", "eJOI", "IOI", "JOI", "RMI", "USACO"]
    with ProcessPoolExecutor(max_workers=11) as executor:
        executor.map(crawl_keyword, keywords)

